# Computer Vision - GPU Accelerated Image Processing
# Demonstrates NVIDIA GPU acceleration for computer vision tasks

import time
import numpy as np
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
from PIL import Image
import cv2

print("üöÄ Computer Vision - GPU Accelerated Processing")
print("=" * 50)

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üéØ Using device: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"CUDA Cores: Thousands of parallel processors")
else:
    print("‚ö†Ô∏è  No GPU detected - running on CPU for comparison")

# Generate the same synthetic image dataset for fair comparison
def create_synthetic_images_gpu(num_images=100, size=(256, 256)):
    """Create synthetic images as GPU tensors"""
    print(f"üì∏ Generating {num_images} synthetic images ({size[0]}x{size[1]}) on GPU...")
    
    # Generate all images at once using GPU vectorization
    images = torch.zeros((num_images, 1, size[0], size[1]), device=device)
    
    # Vectorized image generation on GPU
    for i in range(10):  # Add 10 random circles per image
        centers_x = torch.randint(0, size[1], (num_images,), device=device)
        centers_y = torch.randint(0, size[0], (num_images,), device=device)
        radii = torch.randint(10, 50, (num_images,), device=device)
        intensities = torch.rand((num_images,), device=device)
        
        # Create coordinate grids
        y_grid = torch.arange(size[0], device=device).view(-1, 1).float()
        x_grid = torch.arange(size[1], device=device).view(1, -1).float()
        
        # Vectorized circle drawing
        for j in range(num_images):
            dist = (x_grid - centers_x[j])**2 + (y_grid - centers_y[j])**2
            mask = dist <= radii[j]**2
            images[j, 0][mask] = intensities[j]
    
    # Add noise (vectorized)
    noise = torch.normal(0, 0.1, images.shape, device=device)
    images = torch.clamp(images + noise, 0, 1)
    
    print(f"‚úÖ Generated {num_images} images on GPU")
    return images

# GPU-accelerated image processing operations
class ImageProcessorGPU:
    def __init__(self, device):
        self.device = device
        self.processed_count = 0
    
    def apply_gaussian_blur(self, images, sigma=2.0):
        """Apply Gaussian blur using GPU convolution"""
        # Create Gaussian kernel
        kernel_size = int(2 * np.ceil(3 * sigma) + 1)
        if kernel_size % 2 == 0:
            kernel_size += 1
        
        # Generate 2D Gaussian kernel
        x = torch.arange(kernel_size, device=self.device, dtype=torch.float32)
        x = x - kernel_size // 2
        gauss_1d = torch.exp(-0.5 * (x / sigma) ** 2)
        gauss_1d = gauss_1d / gauss_1d.sum()
        gauss_2d = gauss_1d[:, None] * gauss_1d[None, :]
        gauss_2d = gauss_2d / gauss_2d.sum()
        
        # Apply convolution (GPU accelerated)
        kernel = gauss_2d.unsqueeze(0).unsqueeze(0)
        padding = kernel_size // 2
        
        return F.conv2d(images, kernel, padding=padding)
    
    def apply_edge_detection(self, images):
        """Apply Sobel edge detection using GPU convolution"""
        # Sobel kernels
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                              device=self.device, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                              device=self.device, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        # Apply Sobel filters (parallel GPU execution)
        edges_x = F.conv2d(images, sobel_x, padding=1)
        edges_y = F.conv2d(images, sobel_y, padding=1)
        
        # Compute magnitude
        edges = torch.sqrt(edges_x**2 + edges_y**2)
        return edges
    
    def apply_histogram_equalization(self, images):
        """Apply histogram equalization (GPU optimized)"""
        batch_size = images.shape[0]
        equalized = torch.zeros_like(images)
        
        # Process each image in parallel
        for i in range(batch_size):
            img = images[i, 0]  # Remove channel dimension
            
            # Convert to histogram
            img_flat = (img * 255).long().flatten()
            hist = torch.histc(img_flat.float(), bins=256, min=0, max=255)
            
            # Compute CDF
            cdf = torch.cumsum(hist, dim=0)
            cdf_normalized = cdf * 255 / cdf[-1]
            
            # Apply equalization using GPU indexing
            img_eq = cdf_normalized[img_flat]
            equalized[i, 0] = img_eq.reshape(img.shape) / 255.0
        
        return equalized
    
    def apply_rotation(self, images, angle):
        """Apply rotation using GPU-accelerated affine transformation"""
        # Create rotation matrix
        angle_rad = torch.tensor(angle * np.pi / 180, device=self.device)
        cos_a = torch.cos(angle_rad)
        sin_a = torch.sin(angle_rad)
        
        # Rotation matrix for torchvision
        rotation_matrix = torch.tensor([[cos_a, -sin_a, 0],
                                       [sin_a, cos_a, 0]], device=self.device)
        rotation_matrix = rotation_matrix.unsqueeze(0).repeat(images.shape[0], 1, 1)
        
        # Apply affine transformation (GPU accelerated)
        grid = F.affine_grid(rotation_matrix, images.size(), align_corners=False)
        rotated = F.grid_sample(images, grid, align_corners=False)
        
        return rotated
    
    def extract_features_conv(self, images):
        """Extract features using GPU-accelerated convolutions"""
        # Simple feature extraction using multiple convolution kernels
        # (simplified version of HOG-like features)
        
        features_list = []
        
        # Define multiple edge detection kernels
        kernels = torch.tensor([
            # Horizontal edges
            [[-1, -1, -1], [2, 2, 2], [-1, -1, -1]],
            # Vertical edges  
            [[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]],
            # Diagonal edges
            [[2, -1, -1], [-1, 2, -1], [-1, -1, 2]],
            # Anti-diagonal edges
            [[-1, -1, 2], [-1, 2, -1], [2, -1, -1]]
        ], device=self.device, dtype=torch.float32).unsqueeze(1)
        
        # Apply all kernels in parallel (GPU acceleration)
        for kernel in kernels:
            feature_map = F.conv2d(images, kernel, padding=1)
            # Global average pooling for each feature map
            pooled = F.adaptive_avg_pool2d(feature_map, (1, 1))
            features_list.append(pooled.squeeze())
        
        # Concatenate all features
        features = torch.stack(features_list, dim=1)
        return features
    
    def process_batch_gpu(self, images):
        """Process entire batch on GPU simultaneously"""
        # All operations performed in parallel on GPU
        with torch.no_grad():
            # Apply all transformations to entire batch at once
            blurred = self.apply_gaussian_blur(images, sigma=1.5)
            edges = self.apply_edge_detection(blurred)
            equalized = self.apply_histogram_equalization(edges)
            rotated = self.apply_rotation(equalized, angle=15)
            
            # Extract features for entire batch
            features = self.extract_features_conv(rotated)
            
            self.processed_count += images.shape[0]
            
            return {
                'blurred': blurred,
                'edges': edges,
                'equalized': equalized,
                'rotated': rotated,
                'features': features
            }

# Generate test dataset on GPU
print("\nüìä Creating test dataset on GPU...")
start_gen = time.time()
images_gpu = create_synthetic_images_gpu(num_images=50, size=(256, 256))
gen_time = time.time() - start_gen

print(f"Dataset shape: {images_gpu.shape}")
print(f"Total pixels to process: {np.prod(images_gpu.shape):,}")
print(f"Dataset generation time: {gen_time:.3f} seconds")

# Initialize GPU processor
processor_gpu = ImageProcessorGPU(device)

# Perform GPU-accelerated image processing
print("\nüöÄ Starting GPU-accelerated image processing...")
start_time = time.time()

# Process all images in larger batches (GPU can handle this efficiently)
batch_size = 25  # Larger batches for GPU
all_results_gpu = []
all_features_gpu = []

for i in range(0, len(images_gpu), batch_size):
    batch_start = time.time()
    batch = images_gpu[i:i+batch_size]
    
    print(f"\nProcessing GPU batch {i//batch_size + 1}/{(len(images_gpu) + batch_size - 1)//batch_size}")
    print(f"Batch size: {batch.shape[0]} images")
    
    # Process entire batch simultaneously on GPU
    results = processor_gpu.process_batch_gpu(batch)
    
    all_results_gpu.append(results)
    all_features_gpu.append(results['features'])
    
    batch_time = time.time() - batch_start
    print(f"GPU batch processing time: {batch_time:.3f} seconds")
    print(f"GPU throughput: {batch.shape[0]/batch_time:.1f} images/second")

# Combine all features
all_features_combined = torch.cat(all_features_gpu, dim=0)
total_time = time.time() - start_time

# Performance analysis
print("\nüìà GPU Processing Results:")
print(f"Total processing time: {total_time:.3f} seconds")
print(f"Images processed: {len(images_gpu)}")
print(f"Average time per image: {total_time/len(images_gpu):.6f} seconds")
print(f"GPU throughput: {len(images_gpu)/total_time:.1f} images/second")

# Feature analysis
print(f"\nüîç Feature Extraction Results:")
print(f"Features extracted per image: {all_features_combined.shape[1]}")
print(f"Total features computed: {np.prod(all_features_combined.shape):,}")

# Memory usage
if torch.cuda.is_available():
    memory_mb = torch.cuda.max_memory_allocated() / (1024**2)
    print(f"GPU Memory used: {memory_mb:.1f} MB")

print("\nüöÄ GPU Acceleration Benefits:")
print("‚úÖ Massive parallel processing (thousands of CUDA cores)")
print("‚úÖ Optimized tensor operations with cuDNN")
print("‚úÖ High memory bandwidth for image data")
print("‚úÖ Simultaneous batch processing")
print("‚úÖ Hardware-accelerated convolutions")
print("‚úÖ Parallel pixel-level operations")

# Performance comparison
estimated_cpu_time = 15.0  # Rough estimate based on CPU version
if total_time > 0:
    speedup = estimated_cpu_time / total_time
    print(f"\nüí° Estimated speedup: {speedup:.1f}x faster than CPU!")
    print(f"GPU: {total_time:.3f}s vs estimated CPU: {estimated_cpu_time:.1f}s")

print(f"\nüíæ Processed {processor_gpu.processed_count} images total on GPU")

print("\nüéØ Key Computer Vision GPU Advantages:")
print("1. Parallel pixel processing across thousands of cores")
print("2. Optimized convolution operations (cuDNN)")
print("3. High-bandwidth memory for large image datasets")
print("4. Batch processing of multiple images simultaneously")
print("5. Hardware-accelerated mathematical operations")
print("6. Reduced data transfer overhead with tensor operations")
